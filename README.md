```markdown
# ğŸ† Titanic ML MLOps Pipeline

This repository demonstrates a reproducible ML pipeline for the Titanic dataset using **DVC** for data and pipeline versioning, **MLflow** for experiment tracking, **PySpark** for preprocessing, and **FastAPI + Docker** for lightweight, scalable API deployment.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ api/                  # FastAPI app & Dockerfile
â”œâ”€â”€ configs/              # Project configs (MLflow, etc.)
â”œâ”€â”€ data/                 # DVC-tracked data (not in Git)
â”œâ”€â”€ dvc.yaml              # DVC pipeline definition
â”œâ”€â”€ params.yaml           # All pipeline parameters (urls, etc.)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ scripts/              # Data download & preprocessing scripts
â”œâ”€â”€ features/             # Preprocessed features (DVC-managed)
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Environment Setup

### Option 1: Conda (Recommended)

```
conda create -n mlops-pipeline python=3.10 -y
conda activate mlops-pipeline
pip install --upgrade pip
pip install -r requirements.txt
```

### Option 2: Virtualenv

```
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ“Š MLflow Tracking Setup

**Start a local MLflow tracking server (for experiment logging):**
```
mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 127.0.0.1 --port 5000
```
Open the UI at: [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸ—„ï¸ DVC Data Versioning (Remote, Optional)

**Set up a local DVC remote (if not done already):**
```
mkdir -p dvc_store
dvc remote add -d local ./dvc_store
```

---

## ğŸš¦ Pipeline Reproducibility

Reproduce the full pipeline (download & preprocess):
```
dvc repro
```

**Push outputs to DVC remote:**
```
dvc push
```

---

## ğŸš¢ Build & Launch the API (FastAPI + Docker)

### Build the Docker image:
```
docker build -t titanic-api -f api/Dockerfile .
```

### Run the container:
```
docker run -p 8000:8000 titanic-api
```

Open [http://127.0.0.1:8000/](http://127.0.0.1:8000/) to verify the API is running.

---

## ğŸ§‘â€ğŸ’» Pipeline Details

- **get_data:** Downloads the Titanic CSV to `data/raw` using parameters in `params.yaml`
- **preprocess (PySpark):** Reads from `data/raw/`, cleans and encodes features, writes processed data to `features/` in Parquet format

All **parameter values** (like URLs, output directories) are managed in `params.yaml`.

---

## ğŸ”Œ API Scaffold Example

**File:** `api/app.py`
```
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def root():
    return {"message": "API is up and running"}
```
_Real model inference endpoints can be added as the project grows._

---

## ğŸ§¹ Repo Hygiene & Submission Checklist

- [x] All pipeline/config files tracked: `requirements.txt`, `configs/mlflow.yaml`, `dvc.yaml`, `params.yaml`, `api/Dockerfile`, scripts
- [x] Large data, model outputs only tracked by DVC (never Git)
- [x] All code & commands documented for rapid demo
- [x] Minimal API endpoint available via Docker

---

## ğŸ’¡ Demo Flow (10 minutes)

1. Clone repo & enter project folder
2. Set up environment (`conda ...` or `venv ...`)
3. (Optional) Start `mlflow server` in a new terminal
4. `dvc pull` (if collaborating)
5. `dvc repro` (runs pipeline)
6. `dvc push` (pushes data to remote)
7. `docker build` and `docker run` the API
8. Open browser to API root endpoint
9. (Optional) View MLflow UI for experiment logs

---

## ğŸ“ Contact

Maintainer: **[Keerthivas M]**  
Email: [ch24m538@smail.iitm.ac.in]

---
```

